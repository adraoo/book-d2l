<!--
Documentation: French translation of Chapter 1 of the D2L book.
-->

# 1. Introduction

Il y a quelques années encore, il n'y avait pas de légions de chercheurs en apprentissage profond (_deep learning_) développant des produits et services intelligents dans les grandes entreprises et les startups. Lorsque nous sommes entrés dans le domaine, l'apprentissage automatique (_machine learning_) ne faisait pas les gros titres des journaux quotidiens. Nos parents n'avaient aucune idée de ce qu'était l'apprentissage automatique, et encore moins pourquoi nous pourrions le préférer à une carrière en médecine ou en droit. L'apprentissage automatique était une discipline académique un peu rêveuse dont l'importance industrielle se limitait à un ensemble restreint d'applications réelles, notamment la reconnaissance vocale et la vision par ordinateur. De plus, beaucoup de ces applications nécessitaient tellement de connaissances du domaine qu'elles étaient souvent considérées comme des domaines entièrement séparés pour lesquels l'apprentissage automatique n'était qu'un petit composant. À cette époque, les réseaux de neurones — les prédécesseurs des méthodes d'apprentissage profond sur lesquelles nous nous concentrons dans ce livre — étaient généralement considérés comme démodés.

Pourtant, en quelques années seulement, l'apprentissage profond a pris le monde par surprise, entraînant des progrès rapides dans des domaines aussi divers que la vision par ordinateur, le traitement du langage naturel, la reconnaissance vocale automatique, l'apprentissage par renforcement et l'informatique biomédicale. De plus, le succès de l'apprentissage profond dans tant de tâches d'intérêt pratique a même catalysé des développements en apprentissage automatique théorique et en statistiques. Avec ces avancées en main, nous pouvons désormais construire des voitures qui se conduisent elles-mêmes avec plus d'autonomie que jamais auparavant (bien que moins d'autonomie que certaines entreprises voudraient vous faire croire), des systèmes de dialogue qui déboguent le code en posant des questions de clarification, et des agents logiciels battant les meilleurs joueurs humains du monde à des jeux de société tels que le Go, un exploit que l'on pensait autrefois être à des décennies de distance. Déjà, ces outils exercent une influence de plus en plus large sur l'industrie et la société, changeant la façon dont les films sont réalisés, les maladies sont diagnostiquées, et jouant un rôle croissant dans les sciences fondamentales — de l'astrophysique à la modélisation climatique, en passant par la prévision météorologique et la biomédecine.

## 1.1 Un exemple motivant

Avant de commencer à écrire, les auteurs de ce livre, comme une grande partie de la main-d'œuvre, ont dû se caféiner. Nous avons sauté dans la voiture et commencé à conduire. Utilisant un iPhone, Alex a crié « Dis Siri », réveillant le système de reconnaissance vocale du téléphone. Puis Mu a commandé « itinéraire vers le café Blue Bottle ». Le téléphone a rapidement affiché la transcription de sa commande. Il a également reconnu que nous demandions un itinéraire et a lancé l'application Plans pour répondre à notre requête. Une fois lancée, l'application Plans a identifié un certain nombre d'itinéraires. À côté de chaque itinéraire, le téléphone affichait un temps de trajet prévu. Bien que cette histoire ait été fabriquée pour la commodité pédagogique, elle démontre qu'en l'espace de quelques secondes seulement, nos interactions quotidiennes avec un smartphone peuvent engager plusieurs modèles d'apprentissage automatique.

Imaginez simplement écrire un programme pour répondre à un mot de réveil tel que « Alexa », « OK Google » et « Dis Siri ». Essayez de le coder dans une pièce tout seul avec rien d'autre qu'un ordinateur et un éditeur de code. Comment écririez-vous un tel programme à partir des premiers principes ? Pensez-y... le problème est difficile. Chaque seconde, le microphone collectera environ 44 000 échantillons. Chaque échantillon est une mesure de l'amplitude de l'onde sonore. Quelle règle pourrait mapper de manière fiable un extrait audio brut à des prédictions confiantes {oui, non} sur la présence du mot de réveil dans l'extrait ? Si vous êtes bloqué, ne vous inquiétez pas. Nous ne savons pas non plus comment écrire un tel programme à partir de zéro. C'est pourquoi nous utilisons l'apprentissage automatique.

![Identifier un mot de réveil.](../img/wake-word.svg)

Voici l'astuce. Souvent, même lorsque nous ne savons pas comment dire explicitement à un ordinateur comment mapper des entrées vers des sorties, nous sommes néanmoins capables d'accomplir l'exploit cognitif nous-mêmes. En d'autres termes, même si vous ne savez pas comment programmer un ordinateur pour reconnaître le mot « Alexa », vous êtes vous-même capable de le reconnaître. Forts de cette capacité, nous pouvons collecter un énorme jeu de données contenant des exemples d'extraits audio et des étiquettes associées, indiquant quels extraits contiennent le mot de réveil. Dans l'approche actuellement dominante de l'apprentissage automatique, nous n'essayons pas de concevoir explicitement un système pour reconnaître les mots de réveil. Au lieu de cela, nous définissons un programme flexible dont le comportement est déterminé par un certain nombre de paramètres. Ensuite, nous utilisons le jeu de données pour déterminer les meilleures valeurs de paramètres possibles, c'est-à-dire celles qui améliorent la performance de notre programme par rapport à une mesure de performance choisie.

Vous pouvez penser aux paramètres comme des boutons que nous pouvons tourner, manipulant le comportement du programme. Une fois les paramètres fixés, nous appelons le programme un _modèle_. L'ensemble de tous les programmes distincts (mappages entrée-sortie) que nous pouvons produire simplement en manipulant les paramètres est appelé une _famille de modèles_. Et le « méta-programme » qui utilise notre jeu de données pour choisir les paramètres est appelé un _algorithme d'apprentissage_.

Avant de pouvoir aller de l'avant et engager l'algorithme d'apprentissage, nous devons définir le problème précisément, en déterminant la nature exacte des entrées et des sorties, et en choisissant une famille de modèles appropriée. Dans ce cas, notre modèle reçoit un extrait audio en entrée, et le modèle génère une sélection parmi {oui, non} en sortie. Si tout se passe comme prévu, les suppositions du modèle seront généralement correctes quant à savoir si l'extrait contient le mot de réveil.

Si nous choisissons la bonne famille de modèles, il devrait exister un réglage des boutons tel que le modèle déclenche « oui » chaque fois qu'il entend le mot « Alexa ». Parce que le choix exact du mot de réveil est arbitraire, nous aurons probablement besoin d'une famille de modèles suffisamment riche pour que, via un autre réglage des boutons, elle puisse déclencher « oui » uniquement en entendant le mot « Abricot ». Nous nous attendons à ce que la même famille de modèles convienne pour la reconnaissance d'« Alexa » et d'« Abricot » car elles semblent, intuitivement, être des tâches similaires. Cependant, nous pourrions avoir besoin d'une famille de modèles entièrement différente si nous voulions traiter des entrées ou des sorties fondamentalement différentes, par exemple si nous voulions mapper des images à des légendes, ou des phrases anglaises à des phrases chinoises.

Comme vous pouvez le deviner, si nous réglons simplement tous les boutons au hasard, il est peu probable que notre modèle reconnaisse « Alexa », « Abricot » ou tout autre mot anglais. En apprentissage automatique, l'_apprentissage_ est le processus par lequel nous découvrons le bon réglage des boutons pour forcer le comportement souhaité de notre modèle. En d'autres termes, nous _entraînons_ notre modèle avec des données. Le processus d'entraînement ressemble généralement à ce qui suit :

1. Commencer avec un modèle initialisé aléatoirement qui ne peut rien faire d'utile.
2. Prendre certaines de vos données (par exemple, des extraits audio et les étiquettes {oui, non} correspondantes).
3. Ajuster les boutons pour que le modèle performe mieux tel qu'évalué sur ces exemples.
4. Répéter les étapes 2 et 3 jusqu'à ce que le modèle soit génial.

![Un processus d'entraînement typique.](../img/training-process.svg)

Pour résumer, plutôt que de coder un reconnaisseur de mots de réveil, nous codons un programme qui peut _apprendre_ à reconnaître des mots de réveil, s'il est présenté avec un grand jeu de données étiqueté. Vous pouvez penser à cet acte de déterminer le comportement d'un programme en lui présentant un jeu de données comme une _programmation avec des données_. C'est-à-dire que nous pouvons « programmer » un détecteur de chats en fournissant à notre système d'apprentissage automatique de nombreux exemples de chats et de chiens. De cette façon, le détecteur finira par apprendre à émettre un très grand nombre positif si c'est un chat, un très grand nombre négatif si c'est un chien, et quelque chose de plus proche de zéro s'il n'est pas sûr. Cela effleure à peine la surface de ce que l'apprentissage automatique peut faire. L'apprentissage profond, que nous expliquerons plus en détail plus tard, n'est qu'une méthode parmi tant d'autres populaires pour résoudre des problèmes d'apprentissage automatique.

## 1.2 Composants clés

Dans notre exemple de mot de réveil, nous avons décrit un jeu de données composé d'extraits audio et d'étiquettes binaires, et nous avons donné une idée vague de la façon dont nous pourrions entraîner un modèle pour approximer un mappage des extraits aux classifications. Ce genre de problème, où nous essayons de prédire une étiquette inconnue désignée basée sur des entrées connues étant donné un jeu de données composé d'exemples pour lesquels les étiquettes sont connues, est appelé _apprentissage supervisé_. Ce n'est qu'une parmi de nombreuses sortes de problèmes d'apprentissage automatique. Avant d'explorer d'autres variétés, nous aimerions faire la lumière sur certains composants centraux qui nous suivront partout, quel que soit le type de problème d'apprentissage automatique auquel nous nous attaquons :

1. Les **données** dont nous pouvons apprendre.
2. Un **modèle** de la façon de transformer les données.
3. Une **fonction objectif** qui quantifie à quel point le modèle se débrouille bien (ou mal).
4. Un **algorithme** pour ajuster les paramètres du modèle afin d'optimiser la fonction objectif.

### 1.2.1 Données

Il va peut-être sans dire que vous ne pouvez pas faire de science des données sans données. Nous pourrions perdre des centaines de pages à réfléchir à ce qu'est précisément une donnée, mais pour l'instant, nous nous concentrerons sur les propriétés clés des jeux de données qui nous concerneront. Généralement, nous sommes concernés par une collection d'_exemples_. Pour travailler utilement avec des données, nous avons généralement besoin de trouver une représentation numérique appropriée. Chaque _exemple_ (ou point de données, instance de données, échantillon) consiste généralement en un ensemble d'attributs appelés _caractéristiques_ (parfois appelés covariables ou entrées), sur la base desquels le modèle doit faire ses prédictions. Dans les problèmes d'apprentissage supervisé, notre objectif est de prédire la valeur d'un attribut spécial, appelé l'_étiquette_ (ou cible), qui ne fait pas partie de l'entrée du modèle.

Si nous travaillions avec des données d'image, chaque exemple pourrait consister en une photographie individuelle (les caractéristiques) et un nombre indiquant la catégorie à laquelle la photographie appartient (l'étiquette). La photographie serait représentée numériquement comme trois grilles de valeurs numériques représentant la luminosité de la lumière rouge, verte et bleue à chaque emplacement de pixel. Par exemple, une photographie couleur de (200 \times 200) pixels consisterait en (200 \times 200 \times 3 = 120 000) valeurs numériques.

Alternativement, nous pourrions travailler avec des données de dossiers de santé électroniques et nous attaquer à la tâche de prédire la probabilité qu'un patient donné survive les 30 prochains jours. Ici, nos caractéristiques pourraient consister en une collection d'attributs facilement disponibles et de mesures fréquemment enregistrées, y compris l'âge, les signes vitaux, les comorbidités, les médicaments actuels et les procédures récentes. L'étiquette disponible pour l'entraînement serait une valeur binaire indiquant si chaque patient dans les données historiques a survécu dans la fenêtre de 30 jours.

Dans de tels cas, lorsque chaque exemple est caractérisé par le même nombre de caractéristiques numériques, nous disons que les entrées sont des vecteurs de longueur fixe et nous appelons la longueur (constante) des vecteurs la _dimensionnalité_ des données. Comme vous pouvez l'imaginer, les entrées de longueur fixe peuvent être pratiques, nous donnant une complication de moins à gérer. Cependant, toutes les données ne peuvent pas être facilement représentées comme des vecteurs de longueur fixe. Alors que nous pourrions nous attendre à ce que des images de microscope proviennent d'équipements standard, nous ne pouvons pas nous attendre à ce que des images extraites d'Internet aient toutes la même résolution ou forme. Pour les images, nous pourrions envisager de les recadrer à une taille standard, mais cette stratégie ne nous mène pas très loin. Nous risquons de perdre des informations dans les parties recadrées. De plus, les données textuelles résistent encore plus obstinément aux représentations de longueur fixe. Considérez les avis clients laissés sur des sites de commerce électronique tels qu'Amazon, IMDb et TripAdvisor. Certains sont courts : « c'est nul ! ». D'autres divaguent sur des pages. Un avantage majeur de l'apprentissage profond par rapport aux méthodes traditionnelles est la grâce comparative avec laquelle les modèles modernes peuvent gérer des données de longueur variable.

Généralement, plus nous avons de données, plus notre travail devient facile. Lorsque nous avons plus de données, nous pouvons entraîner des modèles plus puissants et moins compter sur des hypothèses préconçues. Le changement de régime de (comparativement) petites à grandes données est un contributeur majeur au succès de l'apprentissage profond moderne. Pour enfoncer le clou, beaucoup des modèles les plus passionnants en apprentissage profond ne fonctionnent pas sans grands jeux de données. D'autres pourraient fonctionner dans le régime des petites données, mais ne sont pas meilleurs que les approches traditionnelles.

Enfin, il ne suffit pas d'avoir beaucoup de données et de les traiter intelligemment. Nous avons besoin des _bonnes_ données. Si les données sont pleines d'erreurs, ou si les caractéristiques choisies ne sont pas prédictives de la quantité cible d'intérêt, l'apprentissage va échouer. La situation est bien capturée par le cliché : _garbage in, garbage out_ (ordures en entrée, ordures en sortie). De plus, une mauvaise performance prédictive n'est pas la seule conséquence potentielle. Dans les applications sensibles de l'apprentissage automatique, comme la police prédictive, le filtrage de CV et les modèles de risque utilisés pour les prêts, nous devons être particulièrement alertes aux conséquences de données de mauvaise qualité. Un mode d'échec courant concerne les jeux de données où certains groupes de personnes sont sous-représentés dans les données d'entraînement. Imaginez appliquer un système de reconnaissance du cancer de la peau qui n'a jamais vu de peau noire auparavant. L'échec peut également se produire lorsque les données ne sous-représentent pas seulement certains groupes mais reflètent des préjugés sociétaux. Par exemple, si les décisions d'embauche passées sont utilisées pour entraîner un modèle prédictif qui sera utilisé pour filtrer les CV, alors les modèles d'apprentissage automatique pourraient capturer et automatiser par inadvertance des injustices historiques. Notez que tout cela peut arriver sans que le scientifique des données ne conspire activement, ou n'en soit même conscient.

### 1.2.2 Modèles

La plupart de l'apprentissage automatique implique de transformer les données dans un certain sens. Nous pourrions vouloir construire un système qui ingère des photos et prédit le niveau de sourire. Alternativement, nous pourrions vouloir ingérer un ensemble de lectures de capteurs et prédire à quel point les lectures sont normales vs anormales. Par _modèle_, nous désignons la machinerie computationnelle pour ingérer des données d'un type, et recracher des prédictions d'un type potentiellement différent. En particulier, nous nous intéressons aux modèles statistiques qui peuvent être estimés à partir de données. Bien que des modèles simples soient parfaitement capables de traiter des problèmes simples appropriés, les problèmes sur lesquels nous nous concentrons dans ce livre repoussent les limites des méthodes classiques. L'apprentissage profond se différencie des approches classiques principalement par l'ensemble de modèles puissants sur lesquels il se concentre. Ces modèles consistent en de nombreuses transformations successives des données qui sont enchaînées de haut en bas, d'où le nom d'_apprentissage profond_. En chemin pour discuter des modèles profonds, nous discuterons également de certaines méthodes plus traditionnelles.

### 1.2.3 Fonctions objectifs

Plus tôt, nous avons introduit l'apprentissage automatique comme l'apprentissage par l'expérience. Par _apprentissage_ ici, nous entendons l'amélioration à une tâche au fil du temps. Mais qui peut dire ce qui constitue une amélioration ? Vous pourriez imaginer que nous pourrions proposer de mettre à jour notre modèle, et certaines personnes pourraient ne pas être d'accord sur le fait que notre proposition constitue une amélioration ou non.

Afin de développer un système mathématique formel de machines apprenantes, nous devons avoir des mesures formelles de la qualité (ou de la médiocrité) de nos modèles. En apprentissage automatique, et en optimisation plus généralement, nous appelons cela des _fonctions objectifs_. Par convention, nous définissons généralement les fonctions objectifs de sorte que plus bas soit meilleur. Ce n'est qu'une convention. Vous pouvez prendre n'importe quelle fonction pour laquelle plus haut est meilleur, et la transformer en une nouvelle fonction qui est qualitativement identique mais pour laquelle plus bas est meilleur en inversant le signe. Parce que nous choisissons que plus bas soit meilleur, ces fonctions sont parfois appelées _fonctions de perte_.

Lorsque nous essayons de prédire des valeurs numériques, la fonction de perte la plus courante est l'erreur quadratique, c'est-à-dire le carré de la différence entre la prédiction et la cible de vérité terrain. Pour la classification, l'objectif le plus courant est de minimiser le taux d'erreur, c'est-à-dire la fraction d'exemples sur lesquels nos prédictions sont en désaccord avec la vérité terrain. Certains objectifs (par exemple, l'erreur quadratique) sont faciles à optimiser, tandis que d'autres (par exemple, le taux d'erreur) sont difficiles à optimiser directement, en raison de la non-différentiabilité ou d'autres complications. Dans ces cas, il est courant d'optimiser à la place un _objectif de substitution_.

Pendant l'optimisation, nous pensons à la perte comme une fonction des paramètres du modèle, et traitons le jeu de données d'entraînement comme une constante. Nous apprenons les meilleures valeurs des paramètres de notre modèle en minimisant la perte encourue sur un ensemble composé d'un certain nombre d'exemples collectés pour l'entraînement. Cependant, bien faire sur les données d'entraînement ne garantit pas que nous ferons bien sur des données non vues. Donc, nous voudrons généralement diviser les données disponibles en deux partitions : le _jeu de données d'entraînement_ (ou ensemble d'entraînement), pour apprendre les paramètres du modèle ; et le _jeu de données de test_ (ou ensemble de test), qui est gardé pour l'évaluation. À la fin de la journée, nous rapportons généralement comment nos modèles performent sur les deux partitions. Vous pourriez penser à la performance d'entraînement comme analogue aux scores qu'un étudiant obtient aux examens blancs utilisés pour se préparer à un véritable examen final. Même si les résultats sont encourageants, cela ne garantit pas le succès à l'examen final. Au cours des études, l'étudiant pourrait commencer à mémoriser les questions d'entraînement, semblant maîtriser le sujet mais échouant face à des questions non vues précédemment lors de l'examen final réel. Lorsqu'un modèle performe bien sur l'ensemble d'entraînement mais échoue à généraliser aux données non vues, nous disons qu'il _surapprend_ (_overfitting_) les données d'entraînement.

### 1.2.4 Algorithmes d'optimisation

Une fois que nous avons une source de données et une représentation, un modèle et une fonction objectif bien définie, nous avons besoin d'un algorithme capable de rechercher les meilleurs paramètres possibles pour minimiser la fonction de perte. Les algorithmes d'optimisation populaires pour l'apprentissage profond sont basés sur une approche appelée _descente de gradient_. En bref, à chaque étape, cette méthode vérifie, pour chaque paramètre, comment cette perte sur l'ensemble d'entraînement changerait si vous perturbiez ce paramètre d'une petite quantité. Elle mettrait ensuite à jour le paramètre dans la direction qui abaisse la perte.

## 1.3 Types de problèmes d'apprentissage automatique

Le problème du mot de réveil dans notre exemple motivant n'est qu'un parmi tant d'autres que l'apprentissage automatique peut aborder. Pour motiver davantage le lecteur et nous fournir un langage commun qui nous suivra tout au long du livre, nous fournissons maintenant un large aperçu du paysage des problèmes d'apprentissage automatique.

### 1.3.1 Apprentissage supervisé

L'_apprentissage supervisé_ décrit les tâches où l'on nous donne un jeu de données contenant à la fois des caractéristiques et des étiquettes et où l'on nous demande de produire un modèle qui prédit les étiquettes lorsqu'on lui donne des caractéristiques d'entrée. Chaque paire caractéristique-étiquette est appelée un _exemple_. Parfois, lorsque le contexte est clair, nous pouvons utiliser le terme _exemples_ pour faire référence à une collection d'entrées, même lorsque les étiquettes correspondantes sont inconnues. La supervision entre en jeu parce que, pour choisir les paramètres, nous (les superviseurs) fournissons au modèle un jeu de données composé d'exemples étiquetés.

En termes probabilistes, nous sommes généralement intéressés par l'estimation de la probabilité conditionnelle d'une étiquette étant donné des caractéristiques d'entrée. Bien que ce ne soit qu'un paradigme parmi plusieurs, l'apprentissage supervisé représente la majorité des applications réussies de l'apprentissage automatique dans l'industrie. C'est en partie parce que de nombreuses tâches importantes peuvent être décrites clairement comme l'estimation de la probabilité de quelque chose d'inconnu étant donné un ensemble particulier de données disponibles :

- Prédire cancer vs pas cancer, étant donné une image de tomographie par ordinateur.
- Prédire la traduction correcte en français, étant donné une phrase en anglais.
- Prédire le prix d'une action le mois prochain sur la base des données de rapports financiers de ce mois-ci.

Alors que tous les problèmes d'apprentissage supervisé sont capturés par la description simple « prédire les étiquettes étant donné les caractéristiques d'entrée », l'apprentissage supervisé lui-même peut prendre diverses formes et nécessiter des tonnes de décisions de modélisation, selon (entre autres considérations) le type, la taille et la quantité des entrées et des sorties. Par exemple, nous utilisons différents modèles pour traiter des séquences de longueurs arbitraires et des représentations vectorielles de longueur fixe. Nous visiterons bon nombre de ces problèmes en profondeur tout au long de ce livre.

Informellement, le processus d'apprentissage ressemble à quelque chose comme ceci. D'abord, prenez une grande collection d'exemples pour lesquels les caractéristiques sont connues et sélectionnez-en un sous-ensemble aléatoire, en acquérant les étiquettes de vérité terrain pour chacun. Parfois, ces étiquettes peuvent être des données disponibles qui ont déjà été collectées (par exemple, un patient est-il décédé dans l'année suivante ?) et d'autres fois, nous pourrions avoir besoin d'employer des annotateurs humains pour étiqueter les données (par exemple, assigner des images à des catégories). Ensemble, ces entrées et étiquettes correspondantes constituent l'_ensemble d'entraînement_. Nous introduisons le jeu de données d'entraînement dans un _algorithme d'apprentissage supervisé_, une fonction qui prend en entrée un jeu de données et produit une autre fonction : le _modèle appris_. Enfin, nous pouvons fournir des entrées non vues précédemment au modèle appris, en utilisant ses sorties comme prédictions de l'étiquette correspondante. Le processus complet est dessiné dans la Fig. 1.3.1.

![Apprentissage supervisé.](../img/supervised-learning.svg)

#### Régression

La tâche d'apprentissage supervisé la plus simple à comprendre est peut-être la _régression_. Considérez, par exemple, un ensemble de données récoltées à partir d'une base de données de ventes de maisons. Nous pourrions construire un tableau, dans lequel chaque ligne correspond à une maison différente, et chaque colonne correspond à un attribut pertinent, tel que la superficie d'une maison, le nombre de chambres, le nombre de salles de bain et le nombre de minutes (de marche) jusqu'au centre-ville. Dans ce jeu de données, chaque exemple serait une maison spécifique, et le vecteur de caractéristiques correspondant serait une ligne du tableau. Si vous vivez à New York ou San Francisco, et que vous n'êtes pas le PDG d'Amazon, Google, Microsoft ou Facebook, le vecteur de caractéristiques (superficie, nb. de chambres, nb. de salles de bain, distance de marche) pour votre maison pourrait ressembler à quelque chose comme : [600, 1, 1, 60]. Cependant, si vous vivez à Pittsburgh, cela pourrait ressembler davantage à [3000, 4, 3, 10]. Des vecteurs de caractéristiques de longueur fixe comme celui-ci sont essentiels pour la plupart des algorithmes classiques d'apprentissage automatique.

Ce qui fait d'un problème une régression est en fait la forme de la cible. Dites que vous êtes sur le marché pour une nouvelle maison. Vous pourriez vouloir estimer la juste valeur marchande d'une maison, étant donné certaines caractéristiques comme ci-dessus. Les données ici pourraient consister en des annonces immobilières historiques et les étiquettes pourraient être les prix de vente observés. Lorsque les étiquettes prennent des valeurs numériques arbitraires (même dans un certain intervalle), nous appelons cela un problème de régression. L'objectif est de produire un modèle dont les prédictions approximent étroitement les valeurs réelles des étiquettes.

Beaucoup de problèmes pratiques sont facilement décrits comme des problèmes de régression. Prédire la note qu'un utilisateur attribuera à un film peut être considéré comme un problème de régression et si vous aviez conçu un excellent algorithme pour accomplir cet exploit en 2009, vous auriez pu gagner le prix Netflix d'un million de dollars. Prédire la durée de séjour des patients à l'hôpital est également un problème de régression. Une bonne règle empirique est que tout problème de type _combien ?_ (_how much_ ou _how many_) est susceptible d'être une régression. Par exemple :

- Combien d'heures cette chirurgie prendra-t-elle ?
- Quelle quantité de pluie cette ville aura-t-elle dans les six prochaines heures ?

Même si vous n'avez jamais travaillé avec l'apprentissage automatique auparavant, vous avez probablement travaillé sur un problème de régression de manière informelle. Imaginez, par exemple, que vous ayez fait réparer vos canalisations et que votre entrepreneur ait passé 3 heures à retirer la crasse de vos tuyaux d'égout. Puis il vous a envoyé une facture de 350 dollars. Imaginez maintenant que votre ami ait engagé le même entrepreneur pour 2 heures et ait reçu une facture de 250 dollars. Si quelqu'un vous demandait ensuite à combien s'attendre sur sa prochaine facture d'enlèvement de crasse, vous pourriez faire des hypothèses raisonnables, telles que plus d'heures travaillées coûtent plus de dollars. Vous pourriez également supposer qu'il y a des frais de base et que l'entrepreneur facture ensuite à l'heure. Si ces hypothèses s'avéraient vraies, alors étant donné ces deux exemples de données, vous pourriez déjà identifier la structure de prix de l'entrepreneur : 100 dollars par heure plus 50 dollars pour se présenter chez vous. Si vous avez suivi cela, alors vous comprenez déjà l'idée de haut niveau derrière la régression linéaire.

Dans ce cas, nous pourrions produire les paramètres qui correspondaient exactement aux prix de l'entrepreneur. Parfois, ce n'est pas possible, par exemple si une partie de la variation provient de facteurs au-delà de vos deux caractéristiques. Dans ces cas, nous essaierons d'apprendre des modèles qui minimisent la distance entre nos prédictions et les valeurs observées. Dans la plupart de nos chapitres, nous nous concentrerons sur la minimisation de la fonction de perte d'erreur quadratique. Comme nous le verrons plus tard, cette perte correspond à l'hypothèse que nos données ont été corrompues par un bruit gaussien.

#### Classification

Alors que les modèles de régression sont excellents pour répondre aux questions _combien ?_, beaucoup de problèmes ne s'intègrent pas confortablement dans ce modèle. Considérez, par exemple, une banque qui souhaite développer une fonctionnalité de numérisation de chèques pour son application mobile. Idéalement, le client prendrait simplement une photo d'un chèque et l'application reconnaîtrait automatiquement le texte de l'image. En supposant que nous ayons une certaine capacité à segmenter des patchs d'image correspondant à chaque caractère manuscrit, alors la tâche principale restante serait de déterminer quel caractère parmi un ensemble connu est représenté dans chaque patch d'image. Ces types de problèmes _lequel ?_ sont appelés _classification_ et nécessitent un ensemble d'outils différent de ceux utilisés pour la régression, bien que de nombreuses techniques se transposent.

En classification, nous voulons que notre modèle regarde des caractéristiques, par exemple les valeurs de pixels dans une image, et prédise ensuite à quelle catégorie (parfois appelée une classe) parmi un ensemble discret d'options appartient un exemple. Pour les chiffres manuscrits, nous pourrions avoir dix classes, correspondant aux chiffres 0 à 9. La forme la plus simple de classification est lorsqu'il n'y a que deux classes, un problème que nous appelons _classification binaire_. Par exemple, notre jeu de données pourrait consister en des images d'animaux et nos étiquettes pourraient être les classes {chat, chien}. Alors qu'en régression nous cherchions un régresseur pour sortir une valeur numérique, en classification nous cherchons un _classifieur_, dont la sortie est l'assignation de classe prédite.

Pour des raisons dans lesquelles nous entrerons à mesure que le livre deviendra plus technique, il peut être difficile d'optimiser un modèle qui ne peut sortir qu'une assignation catégorielle ferme, par exemple soit « chat » soit « chien ». Dans ces cas, il est généralement beaucoup plus facile d'exprimer notre modèle dans le langage des probabilités. Étant donné les caractéristiques d'un exemple, notre modèle assigne une probabilité à chaque classe possible. Revenant à notre exemple de classification d'animaux où les classes sont {chat, chien}, un classifieur pourrait voir une image et sortir que la probabilité que l'image soit un chat est de 0,9. Nous pouvons interpréter ce nombre en disant que le classifieur est sûr à 90 % que l'image représente un chat. L'ampleur de la probabilité pour la classe prédite transmet une notion d'incertitude. Ce n'est pas la seule disponible et nous en discuterons d'autres dans les chapitres traitant de sujets plus avancés.

Lorsque nous avons plus de deux classes possibles, nous appelons le problème _classification multiclasse_. Les exemples courants incluent la reconnaissance de caractères manuscrits {0, 1, 2, ... 9, a, b, c, ...}. Alors que nous avons attaqué les problèmes de régression en essayant de minimiser la fonction de perte d'erreur quadratique, la fonction de perte commune pour les problèmes de classification est appelée _entropie croisée_, dont le nom sera démystifié lorsque nous introduirons la théorie de l'information dans les chapitres ultérieurs.

Notez que la classe la plus probable n'est pas nécessairement celle que vous allez utiliser pour votre décision. Supposons que vous trouviez un beau champignon dans votre jardin comme indiqué dans la Fig. 1.3.2.

Maintenant, supposons que vous ayez construit un classifieur et l'ayez entraîné pour prédire si un champignon est toxique sur la base d'une photographie. Disons que notre classifieur de détection de poison sort que la probabilité que la Fig. 1.3.2 montre une amanite phalloïde est de 0,2. En d'autres termes, le classifieur est sûr à 80 % que notre champignon n'est pas une amanite phalloïde. Pourtant, vous devriez être fou pour le manger. C'est parce que le bénéfice certain d'un délicieux dîner ne vaut pas un risque de 20 % d'en mourir. En d'autres termes, l'effet du risque incertain l'emporte de loin sur le bénéfice. Ainsi, pour prendre une décision quant à savoir s'il faut manger le champignon, nous devons calculer le détriment attendu associé à chaque action, qui dépend à la fois des résultats probables et des avantages ou inconvénients associés à chacun. Dans ce cas, le détriment encouru en mangeant le champignon pourrait être (0.2 \times \infty + 0.8 \times 0 = \infty), tandis que la perte de le jeter est (0.2 \times 0 + 0.8 \times 1 = 0.8). Notre prudence était justifiée : comme tout mycologue nous le dirait, le champignon de la Fig. 1.3.2 est en fait une amanite phalloïde.

![Amanite phalloïde - ne pas manger !](../img/death-cap.svg)

La classification peut devenir beaucoup plus compliquée que la simple classification binaire ou multiclasse. Par exemple, il existe des variantes de classification traitant de classes structurées hiérarchiquement. Dans de tels cas, toutes les erreurs ne sont pas égales — si nous devons nous tromper, nous pourrions préférer mal classer vers une classe liée plutôt que vers une classe distante. Généralement, on parle de _classification hiérarchique_. Pour l'inspiration, vous pourriez penser à Linné, qui a organisé la faune en une hiérarchie.

Dans le cas de la classification des animaux, il ne serait peut-être pas si grave de confondre un caniche avec un schnauzer, mais notre modèle paierait une énorme pénalité s'il confondait un caniche avec un dinosaure. Quelle hiérarchie est pertinente peut dépendre de la façon dont vous prévoyez d'utiliser le modèle. Par exemple, les crotales et les couleuvres rayées peuvent être proches sur l'arbre phylogénétique, mais confondre un crotale avec une couleuvre pourrait avoir des conséquences fatales.

#### Étiquetage (Tagging)

Certains problèmes de classification s'intègrent parfaitement dans les configurations de classification binaire ou multiclasse. Par exemple, nous pourrions entraîner un classifieur binaire normal pour distinguer les chats des chiens. Étant donné l'état actuel de la vision par ordinateur, nous pouvons le faire facilement, avec des outils prêts à l'emploi. Néanmoins, quelle que soit la précision de notre modèle, nous pourrions nous retrouver en difficulté lorsque le classifieur rencontre une image des Musiciens de Brême, un conte de fées allemand populaire mettant en vedette quatre animaux (Fig. 1.3.3).

Comme vous pouvez le voir, la photo présente un chat, un coq, un chien et un âne, avec quelques arbres en arrière-plan. Si nous prévoyons de rencontrer de telles images, la classification multiclasse pourrait ne pas être la bonne formulation du problème. Au lieu de cela, nous pourrions vouloir donner au modèle la possibilité de dire que l'image représente un chat, un chien, un âne _et_ un coq.

![Un âne, un chien, un chat et un coq.](../img/bremen-musicians.svg)

Le problème d'apprendre à prédire des classes qui ne sont pas mutuellement exclusives est appelé _classification multi-étiquettes_. Les problèmes d'auto-étiquetage sont généralement mieux décrits en termes de classification multi-étiquettes. Pensez aux étiquettes (tags) que les gens pourraient appliquer aux articles sur un blog technique, par exemple, « machine learning », « technologie », « gadgets », « langages de programmation », « Linux », « cloud computing », « AWS ». Un article typique pourrait avoir 5 à 10 étiquettes appliquées. Généralement, les étiquettes présenteront une certaine structure de corrélation. Les articles sur le « cloud computing » sont susceptibles de mentionner « AWS » et les articles sur le « machine learning » sont susceptibles de mentionner les « GPU ».

Parfois, de tels problèmes d'étiquetage s'appuient sur d'énormes ensembles d'étiquettes. La National Library of Medicine emploie de nombreux annotateurs professionnels qui associent chaque article à indexer dans PubMed à un ensemble d'étiquettes tirées de l'ontologie Medical Subject Headings (MeSH), une collection d'environ 28 000 étiquettes. Étiqueter correctement les articles est important car cela permet aux chercheurs de mener des examens exhaustifs de la littérature. C'est un processus long et il y a généralement un décalage d'un an entre l'archivage et l'étiquetage. L'apprentissage automatique peut fournir des étiquettes provisoires jusqu'à ce que chaque article ait un examen manuel approprié. En effet, depuis plusieurs années, l'organisation BioASQ organise des compétitions pour cette tâche.

#### Recherche

Dans le domaine de la recherche d'information, nous imposons souvent des rangs sur des ensembles d'éléments. Prenez la recherche sur le Web par exemple. L'objectif est moins de déterminer si une page particulière est pertinente pour une requête, mais plutôt laquelle, parmi un ensemble de résultats pertinents, doit être montrée le plus en évidence à un utilisateur particulier. Une façon de faire cela pourrait être d'abord d'attribuer un score à chaque élément de l'ensemble, puis de récupérer les éléments les mieux notés. PageRank, la sauce secrète originale derrière le moteur de recherche Google, était un premier exemple d'un tel système de notation. Curieusement, le scoring fourni par PageRank ne dépendait pas de la requête réelle. Au lieu de cela, ils s'appuyaient sur un simple filtre de pertinence pour identifier l'ensemble des candidats pertinents, puis utilisaient PageRank pour prioriser les pages les plus autoritaires. De nos jours, les moteurs de recherche utilisent l'apprentissage automatique et des modèles comportementaux pour obtenir des scores de pertinence dépendants de la requête. Il existe des conférences académiques entières consacrées à ce sujet.

#### Systèmes de recommandation

Les systèmes de recommandation sont un autre cadre de problème lié à la recherche et au classement. Les problèmes sont similaires dans la mesure où l'objectif est d'afficher un ensemble d'éléments pertinents pour l'utilisateur. La principale différence est l'accent mis sur la _personnalisation_ pour des utilisateurs spécifiques dans le contexte des systèmes de recommandation. Par exemple, pour les recommandations de films, la page de résultats pour un fan de science-fiction et la page de résultats pour un connaisseur des comédies de Peter Sellers pourraient différer considérablement. Des problèmes similaires surgissent dans d'autres contextes de recommandation, par exemple pour les produits de détail, la musique et la recommandation d'actualités.

Dans certains cas, les clients fournissent des commentaires explicites, communiquant à quel point ils ont aimé un produit particulier (par exemple, les évaluations de produits et les avis sur Amazon, IMDb ou Goodreads). Dans d'autres cas, ils fournissent des commentaires implicites, par exemple en sautant des titres sur une liste de lecture, ce qui pourrait indiquer une insatisfaction ou peut-être simplement indiquer que la chanson était inappropriée dans le contexte. Dans les formulations les plus simples, ces systèmes sont entraînés pour estimer un certain score, tel qu'une note attendue ou la probabilité qu'un utilisateur donné achète un article particulier.

Étant donné un tel modèle, pour tout utilisateur donné, nous pourrions récupérer l'ensemble des objets avec les scores les plus élevés, qui pourraient ensuite être recommandés à l'utilisateur. Les systèmes de production sont considérablement plus avancés et prennent en compte l'activité détaillée de l'utilisateur et les caractéristiques des articles lors du calcul de ces scores. La Fig. 1.3.4 affiche les livres d'apprentissage profond recommandés par Amazon sur la base d'algorithmes de personnalisation réglés pour capturer les préférences d'Aston.

![Livres d'apprentissage profond recommandés par Amazon.](../img/amazon-recs.svg)

Malgré leur énorme valeur économique, les systèmes de recommandation construits naïvement sur des modèles prédictifs souffrent de graves défauts conceptuels. Pour commencer, nous n'observons que des retours censurés : les utilisateurs notent préférentiellement les films pour lesquels ils ont un avis tranché. Par exemple, sur une échelle de cinq points, vous pourriez remarquer que les articles reçoivent beaucoup de notes d'une et cinq étoiles, mais qu'il y a remarquablement peu de notes de trois étoiles. De plus, les habitudes d'achat actuelles sont souvent le résultat de l'algorithme de recommandation actuellement en place, mais les algorithmes d'apprentissage ne prennent pas toujours ce détail en compte. Ainsi, il est possible que des boucles de rétroaction se forment où un système de recommandation pousse préférentiellement un article qui est ensuite considéré comme meilleur (en raison de plus grands achats) et est à son tour recommandé encore plus fréquemment. Beaucoup de ces problèmes — sur la façon de gérer la censure, les incitations et les boucles de rétroaction — sont d'importantes questions de recherche ouvertes.

#### Apprentissage de séquences

Jusqu'à présent, nous avons examiné des problèmes où nous avons un nombre fixe d'entrées et produisons un nombre fixe de sorties. Par exemple, nous avons envisagé de prédire les prix des maisons étant donné un ensemble fixe de caractéristiques : superficie, nombre de chambres, nombre de salles de bain et temps de trajet jusqu'au centre-ville. Nous avons également discuté du mappage d'une image (de dimension fixe) aux probabilités prédites qu'elle appartienne à chacune parmi un nombre fixe de classes et de la prédiction des notes associées aux achats basés sur l'ID utilisateur et l'ID produit seuls. Dans ces cas, une fois notre modèle entraîné, après que chaque exemple de test est introduit dans notre modèle, il est immédiatement oublié. Nous avons supposé que les observations successives étaient indépendantes et qu'il n'y avait donc pas besoin de conserver ce contexte.

Mais comment devrions-nous traiter les extraits vidéo ? Dans ce cas, chaque extrait pourrait consister en un nombre différent d'images (_frames_). Et notre supposition de ce qui se passe dans chaque image pourrait être beaucoup plus forte si nous prenons en compte les images précédentes ou suivantes. Il en va de même pour le langage. Par exemple, un problème populaire d'apprentissage profond est la traduction automatique : la tâche d'ingérer des phrases dans une langue source et de prédire leurs traductions dans une autre langue.

De tels problèmes se produisent également en médecine. Nous pourrions vouloir un modèle pour surveiller les patients en unité de soins intensifs et déclencher des alertes chaque fois que leur risque de mourir dans les prochaines 24 heures dépasse un certain seuil. Ici, nous ne jetterions pas tout ce que nous savons sur l'histoire du patient toutes les heures, car nous ne voudrions peut-être pas faire de prédictions basées uniquement sur les mesures les plus récentes.

Des questions comme celles-ci sont parmi les applications les plus passionnantes de l'apprentissage automatique et elles sont des instances d'_apprentissage de séquences_. Elles nécessitent un modèle pour ingérer des séquences d'entrées ou pour émettre des séquences de sorties (ou les deux). Spécifiquement, l'apprentissage _séquence-à-séquence_ considère des problèmes où les entrées et les sorties consistent en des séquences de longueur variable. Les exemples incluent la traduction automatique et la transcription parole-texte. Bien qu'il soit impossible de considérer tous les types de transformations de séquences, les cas spéciaux suivants valent la peine d'être mentionnés.

**Étiquetage et analyse.** Cela implique d'annoter une séquence de texte avec des attributs. Ici, les entrées et les sorties sont alignées, c'est-à-dire qu'elles sont du même nombre et se produisent dans un ordre correspondant. Par exemple, dans l'étiquetage des parties du discours (PoS tagging), nous annotons chaque mot d'une phrase avec la partie du discours correspondante, c'est-à-dire « nom » ou « objet direct ». Alternativement, nous pourrions vouloir savoir quels groupes de mots contigus font référence à des entités nommées, comme des personnes, des lieux ou des organisations. Dans l'exemple caricaturalement simple ci-dessous, nous pourrions simplement vouloir indiquer si un mot dans la phrase fait partie ou non d'une entité nommée (étiquetée comme « Ent »).

```
Tom has dinner in Washington with Sally
Ent  -    -    -     Ent      -    Ent
```

**Reconnaissance vocale automatique.** Avec la reconnaissance vocale, la séquence d'entrée est un enregistrement audio d'un locuteur (Fig. 1.3.5), et la sortie est une transcription de ce que le locuteur a dit. Le défi est qu'il y a beaucoup plus de trames audio (le son est généralement échantillonné à 8 kHz ou 16 kHz) que de texte, c'est-à-dire qu'il n'y a pas de correspondance 1:1 entre l'audio et le texte, puisque des milliers d'échantillons peuvent correspondre à un seul mot parlé. Ce sont des problèmes d'apprentissage séquence-à-séquence, où la sortie est beaucoup plus courte que l'entrée. Bien que les humains soient remarquablement bons pour reconnaître la parole, même à partir d'un son de mauvaise qualité, faire accomplir le même exploit aux ordinateurs est un défi formidable.

![Deep learning dans un enregistrement audio.](../img/speech-audio.svg)

**Texte-à-parole.** C'est l'inverse de la reconnaissance vocale automatique. Ici, l'entrée est du texte et la sortie est un fichier audio. Dans ce cas, la sortie est beaucoup plus longue que l'entrée.

**Traduction automatique.** Contrairement au cas de la reconnaissance vocale, où les entrées et sorties correspondantes se produisent dans le même ordre, en traduction automatique, les données non alignées posent un nouveau défi. Ici, les séquences d'entrée et de sortie peuvent avoir des longueurs différentes, et les régions correspondantes des séquences respectives peuvent apparaître dans un ordre différent. Considérez l'exemple illustratif suivant de la tendance particulière des Allemands à placer les verbes à la fin des phrases :

- Allemand : Haben Sie sich schon dieses grossartige Lehrwerk **angeschaut**?
- Anglais : Have you already **looked** at this excellent textbook?
- Mauvais alignement : Have you yourself already this excellent textbook **looked** at?

De nombreux problèmes connexes surgissent dans d'autres tâches d'apprentissage. Par exemple, déterminer l'ordre dans lequel un utilisateur lit une page Web est un problème d'analyse de mise en page bidimensionnelle. Les problèmes de dialogue présentent toutes sortes de complications supplémentaires, où déterminer quoi dire ensuite nécessite de prendre en compte la connaissance du monde réel et l'état antérieur de la conversation sur de longues distances temporelles. Ces sujets sont des domaines de recherche actifs.

### 1.3.2 Apprentissage non supervisé et auto-supervisé

Les exemples précédents se concentraient sur l'apprentissage supervisé, où nous fournissons au modèle un jeu de données géant contenant à la fois les caractéristiques et les valeurs d'étiquettes correspondantes. Vous pourriez penser à l'apprenant supervisé comme ayant un travail extrêmement spécialisé et un patron extrêmement dictatorial. Le patron se tient au-dessus de l'épaule de l'apprenant et lui dit exactement quoi faire dans chaque situation jusqu'à ce qu'il apprenne à mapper des situations aux actions. Travailler pour un tel patron semble assez nul. D'un autre côté, plaire à un tel patron est assez facile. Vous reconnaissez simplement le modèle le plus rapidement possible et imitez les actions du patron.

Considérant la situation inverse, il pourrait être frustrant de travailler pour un patron qui n'a aucune idée de ce qu'il veut que vous fassiez. Cependant, si vous prévoyez d'être un data scientist, vous feriez mieux de vous y habituer. Le patron pourrait simplement vous remettre un énorme tas de données et vous dire d'en faire de la science des données ! Cela semble vague parce que c'est vague. Nous appelons cette classe de problèmes _apprentissage non supervisé_, et le type et le nombre de questions que nous pouvons poser ne sont limités que par notre créativité. Nous aborderons les techniques d'apprentissage non supervisé dans les chapitres ultérieurs. Pour vous mettre en appétit pour l'instant, nous décrivons quelques-unes des questions suivantes que vous pourriez poser.

- Pouvons-nous trouver un petit nombre de prototypes qui résument avec précision les données ? Étant donné un ensemble de photos, pouvons-nous les regrouper en photos de paysages, photos de chiens, bébés, chats et sommets de montagnes ? De même, étant donné une collection d'activités de navigation d'utilisateurs, pouvons-nous les regrouper en utilisateurs ayant un comportement similaire ? Ce problème est généralement connu sous le nom de _clustering_ (regroupement).
- Pouvons-nous trouver un petit nombre de paramètres qui capturent avec précision les propriétés pertinentes des données ? Les trajectoires d'une balle sont bien décrites par la vitesse, le diamètre et la masse de la balle. Les tailleurs ont développé un petit nombre de paramètres qui décrivent la forme du corps humain assez précisément dans le but d'ajuster les vêtements. Ces problèmes sont appelés _estimation de sous-espace_. Si la dépendance est linéaire, cela s'appelle _analyse en composantes principales_ (PCA).
- Existe-t-il une représentation d'objets (arbitrairement structurés) dans l'espace euclidien telle que les propriétés symboliques puissent être bien appariées ? Cela peut être utilisé pour décrire des entités et leurs relations, telles que « Rome » - « Italie » + « France » = « Paris ».
- Existe-t-il une description des causes profondes d'une grande partie des données que nous observons ? Par exemple, si nous avons des données démographiques sur les prix des maisons, la pollution, la criminalité, la localisation, l'éducation et les salaires, pouvons-nous découvrir comment ils sont liés simplement sur la base de données empiriques ? Les domaines concernés par la _causalité_ et les _modèles graphiques probabilistes_ s'attaquent à de telles questions.
- Un autre développement important et récent en apprentissage non supervisé est l'avènement des _modèles génératifs profonds_. Ces modèles estiment la densité des données, soit explicitement soit implicitement. Une fois entraînés, nous pouvons utiliser un modèle génératif soit pour scorer des exemples selon leur probabilité, soit pour échantillonner des exemples synthétiques à partir de la distribution apprise. Les premières percées de l'apprentissage profond dans la modélisation générative sont venues avec l'invention des _auto-encodeurs variationnels_ (Kingma et Welling, 2014, Rezende et al., 2014) et se sont poursuivies avec le développement des _réseaux antagonistes génératifs_ (GANs) (Goodfellow et al., 2014). Des avancées plus récentes incluent les _flots normalisants_ (Dinh et al., 2014, Dinh et al., 2017) et les _modèles de diffusion_ (Ho et al., 2020, Sohl-Dickstein et al., 2015, Song et Ermon, 2019, Song et al., 2021).

Un autre développement de l'apprentissage non supervisé a été la montée de l'_apprentissage auto-supervisé_, des techniques qui tirent parti d'un aspect des données non étiquetées pour fournir une supervision. Pour le texte, nous pouvons entraîner des modèles à « remplir les blancs » en prédisant des mots masqués aléatoirement en utilisant leurs mots environnants (contextes) dans de grands corpus sans aucun effort d'étiquetage (Devlin et al., 2018) ! Pour les images, nous pouvons entraîner des modèles à indiquer la position relative entre deux régions recadrées de la même image (Doersch et al., 2015), à prédire une partie occluse d'une image basée sur les parties restantes de l'image, ou à prédire si deux exemples sont des versions perturbées de la même image sous-jacente. Les modèles auto-supervisés apprennent souvent des représentations qui sont ensuite exploitées en affinant (_fine-tuning_) les modèles résultants sur une tâche en aval d'intérêt.

### 1.3.3 Interagir avec un environnement

Jusqu'à présent, nous n'avons pas discuté d'où viennent réellement les données, ni de ce qui se passe réellement lorsqu'un modèle d'apprentissage automatique génère une sortie. C'est parce que l'apprentissage supervisé et l'apprentissage non supervisé n'abordent pas ces questions de manière très sophistiquée. Dans chaque cas, nous saisissons un gros tas de données au départ, puis mettons nos machines de reconnaissance de formes en mouvement sans jamais interagir à nouveau avec l'environnement. Parce que tout l'apprentissage a lieu après que l'algorithme est déconnecté de l'environnement, cela est parfois appelé _apprentissage hors ligne_ (_offline learning_). Par exemple, l'apprentissage supervisé suppose le modèle d'interaction simple représenté dans la Fig. 1.3.6.

![Collecte de données pour l'apprentissage supervisé à partir d'un environnement.](../img/data-collection.svg)

Cette simplicité de l'apprentissage hors ligne a ses charmes. L'avantage est que nous pouvons nous soucier de la reconnaissance de formes de manière isolée, sans nous soucier des complications découlant des interactions avec un environnement dynamique. Mais cette formulation du problème est limitative. Si vous avez grandi en lisant les romans de robots d'Asimov, alors vous imaginez probablement des agents artificiellement intelligents capables non seulement de faire des prédictions, mais aussi de prendre des actions dans le monde. Nous voulons penser à des _agents intelligents_, pas seulement à des modèles prédictifs. Cela signifie que nous devons penser à choisir des _actions_, pas seulement à faire des prédictions. Contrairement aux simples prédictions, les actions impactent réellement l'environnement. Si nous voulons entraîner un agent intelligent, nous devons tenir compte de la façon dont ses actions pourraient impacter les futures observations de l'agent, et donc l'apprentissage hors ligne est inapproprié.

Considérer l'interaction avec un environnement ouvre tout un ensemble de nouvelles questions de modélisation. Voici quelques exemples :

- L'environnement se souvient-il de ce que nous avons fait précédemment ?
- L'environnement veut-il nous aider (par exemple, un utilisateur lisant du texte dans un reconnaisseur vocal) ?
- L'environnement veut-il nous battre (par exemple, des spammeurs adaptant leurs e-mails pour échapper aux filtres anti-spam) ?
- L'environnement a-t-il une dynamique changeante ? Par exemple, les données futures ressembleront-elles toujours au passé ou les modèles changeront-ils au fil du temps, soit naturellement, soit en réponse à nos outils automatisés ?

Ces questions soulèvent le problème du _décalage de distribution_ (_distribution shift_), où les données d'entraînement et de test sont différentes. Un exemple de cela, que beaucoup d'entre nous ont peut-être rencontré, est de passer des examens écrits par un conférencier, alors que les devoirs ont été composés par leurs assistants d'enseignement. Ensuite, nous décrivons brièvement l'_apprentissage par renforcement_, un cadre riche pour poser des problèmes d'apprentissage dans lesquels un agent interagit avec un environnement.

### 1.3.4 Apprentissage par renforcement

Si vous êtes intéressé par l'utilisation de l'apprentissage automatique pour développer un agent qui interagit avec un environnement et prend des actions, alors vous allez probablement finir par vous concentrer sur l'apprentissage par renforcement. Cela pourrait inclure des applications à la robotique, aux systèmes de dialogue, et même au développement d'intelligence artificielle (IA) pour les jeux vidéo. L'_apprentissage par renforcement profond_, qui applique l'apprentissage profond aux problèmes d'apprentissage par renforcement, a connu une montée en popularité. Le révolutionnaire _deep Q-network_, qui a battu les humains aux jeux Atari en utilisant uniquement l'entrée visuelle (Mnih et al., 2015), et le programme AlphaGo, qui a détrôné le champion du monde au jeu de société Go (Silver et al., 2016), sont deux exemples marquants.

L'apprentissage par renforcement donne un énoncé très général d'un problème dans lequel un agent interagit avec un environnement sur une série de pas de temps. À chaque pas de temps, l'agent reçoit une observation de l'environnement et doit choisir une action qui est ensuite transmise à l'environnement via un certain mécanisme (parfois appelé un actionneur), quand, après chaque boucle, l'agent reçoit une récompense de l'environnement. Ce processus est illustré dans la Fig. 1.3.7. L'agent reçoit ensuite une observation suivante, et choisit une action suivante, et ainsi de suite. Le comportement d'un agent d'apprentissage par renforcement est régi par une _politique_. En bref, une politique est juste une fonction qui mappe des observations de l'environnement à des actions. L'objectif de l'apprentissage par renforcement est de produire de bonnes politiques.

![L'interaction entre l'apprentissage par renforcement et un environnement.](../img/rl-environment.svg)

Il est difficile d'exagérer la généralité du cadre de l'apprentissage par renforcement. Par exemple, l'apprentissage supervisé peut être reformulé comme un apprentissage par renforcement. Disons que nous avions un problème de classification. Nous pourrions créer un agent d'apprentissage par renforcement avec une action correspondant à chaque classe. Nous pourrions alors créer un environnement qui donnait une récompense qui était exactement égale à la fonction de perte du problème d'apprentissage supervisé original.

De plus, l'apprentissage par renforcement peut également aborder de nombreux problèmes que l'apprentissage supervisé ne peut pas. Par exemple, en apprentissage supervisé, nous nous attendons toujours à ce que l'entrée d'entraînement vienne associée à l'étiquette correcte. Mais en apprentissage par renforcement, nous ne supposons pas que, pour chaque observation, l'environnement nous dit l'action optimale. En général, nous recevons juste une récompense. De plus, l'environnement peut même ne pas nous dire quelles actions ont conduit à la récompense.

Considérez le jeu d'échecs. Le seul véritable signal de récompense vient à la fin du jeu lorsque nous gagnons (gagnant une récompense de, disons, 1), ou lorsque nous perdons (recevant une récompense de, disons, -1). Donc les apprenants par renforcement doivent faire face au _problème d'attribution de crédit_ : déterminer quelles actions créditer ou blâmer pour un résultat. Il en va de même pour un employé qui obtient une promotion le 11 octobre. Cette promotion reflète probablement un certain nombre d'actions bien choisies au cours de l'année précédente. Obtenir une promotion à l'avenir nécessite de comprendre quelles actions en cours de route ont conduit aux promotions précédentes.

Les apprenants par renforcement peuvent également avoir à faire face au problème de l'_observabilité partielle_. C'est-à-dire que l'observation actuelle pourrait ne pas vous dire tout sur votre état actuel. Disons que votre robot de nettoyage s'est retrouvé piégé dans l'un des nombreux placards identiques de votre maison. Sauver le robot implique d'inférer son emplacement précis, ce qui pourrait nécessiter de prendre en compte des observations antérieures avant qu'il n'entre dans le placard.

Enfin, à tout moment donné, les apprenants par renforcement pourraient connaître une bonne politique, mais il pourrait y avoir beaucoup d'autres meilleures politiques que l'agent n'a jamais essayées. L'apprenant par renforcement doit constamment choisir entre _exploiter_ la meilleure stratégie (actuellement) connue comme politique, ou _explorer_ l'espace des stratégies, renonçant potentiellement à une récompense à court terme en échange de connaissances.

Le problème général de l'apprentissage par renforcement a un cadre très général. Les actions affectent les observations suivantes. Les récompenses ne sont observées que lorsqu'elles correspondent aux actions choisies. L'environnement peut être soit entièrement soit partiellement observé. Tenir compte de toute cette complexité à la fois peut être trop demander. De plus, tous les problèmes pratiques ne présentent pas toute cette complexité. En conséquence, les chercheurs ont étudié un certain nombre de cas particuliers de problèmes d'apprentissage par renforcement.

Lorsque l'environnement est entièrement observé, nous appelons le problème d'apprentissage par renforcement un _processus de décision de Markov_. Lorsque l'état ne dépend pas des actions précédentes, nous l'appelons un _problème de bandit contextuel_. Lorsqu'il n'y a pas d'état, juste un ensemble d'actions disponibles avec des récompenses initialement inconnues, nous avons le problème classique du _bandit manchot_.

## 1.4 Racines

Nous venons de passer en revue un petit sous-ensemble de problèmes que l'apprentissage automatique peut aborder. Pour un ensemble diversifié de problèmes d'apprentissage automatique, l'apprentissage profond fournit des outils puissants pour leur solution. Bien que de nombreuses méthodes d'apprentissage profond soient des inventions récentes, les idées fondamentales derrière l'apprentissage à partir de données sont étudiées depuis des siècles. En fait, les humains ont le désir d'analyser des données et de prédire les résultats futurs depuis des lustres, et c'est ce désir qui est à la racine d'une grande partie des sciences naturelles et des mathématiques. Deux exemples sont la distribution de Bernoulli, nommée d'après Jacob Bernoulli (1655–1705), et la distribution gaussienne découverte par Carl Friedrich Gauss (1777–1855). Gauss a inventé, par exemple, l'algorithme des moindres carrés moyens, qui est encore utilisé aujourd'hui pour une multitude de problèmes allant des calculs d'assurance aux diagnostics médicaux. De tels outils ont amélioré l'approche expérimentale dans les sciences naturelles — par exemple, la loi d'Ohm reliant le courant et la tension dans une résistance est parfaitement décrite par un modèle linéaire.

Même au Moyen Âge, les mathématiciens avaient une intuition fine des estimations. Par exemple, le livre de géométrie de Jacob Köbel (1460–1533) illustre la moyenne de la longueur des pieds de 16 hommes adultes pour estimer la longueur typique du pied dans la population (Fig. 1.4.1).

![Estimer la longueur d'un pied.](../img/koebel.svg)

Alors qu'un groupe d'individus sortait d'une église, 16 hommes adultes ont été invités à s'aligner et à faire mesurer leurs pieds. La somme de ces mesures a ensuite été divisée par 16 pour obtenir une estimation de ce qui est maintenant appelé un pied. Cet « algorithme » a ensuite été amélioré pour traiter les pieds déformés ; les deux hommes avec les pieds les plus courts et les plus longs ont été renvoyés, faisant la moyenne uniquement sur le reste. C'est l'un des premiers exemples d'une estimation de la moyenne tronquée.

La statistique a vraiment décollé avec la disponibilité et la collecte de données. L'un de ses pionniers, Ronald Fisher (1890–1962), a contribué de manière significative à sa théorie et aussi à ses applications en génétique. Beaucoup de ses algorithmes (tels que l'analyse discriminante linéaire) et concepts (tels que la matrice d'information de Fisher) occupent encore une place prépondérante dans les fondements des statistiques modernes. Même ses ressources de données ont eu un impact durable. Le jeu de données Iris que Fisher a publié en 1936 est encore parfois utilisé pour démontrer des algorithmes d'apprentissage automatique. Fisher était également un partisan de l'eugénisme, ce qui devrait nous rappeler que l'utilisation moralement douteuse de la science des données a une histoire aussi longue et durable que son utilisation productive dans l'industrie et les sciences naturelles.

D'autres influences pour l'apprentissage automatique sont venues de la théorie de l'information de Claude Shannon (1916–2001) et de la théorie du calcul proposée par Alan Turing (1912–1954). Turing a posé la question « les machines peuvent-elles penser ? » dans son célèbre article _Computing Machinery and Intelligence_ (Turing, 1950). Décrivant ce qui est maintenant connu sous le nom de test de Turing, il a proposé qu'une machine puisse être considérée comme intelligente s'il est difficile pour un évaluateur humain de distinguer les réponses d'une machine de celles d'un humain, basées uniquement sur des interactions textuelles.

D'autres influences sont venues des neurosciences et de la psychologie. Après tout, les humains présentent clairement un comportement intelligent. De nombreux chercheurs ont demandé si l'on pouvait expliquer et éventuellement faire de l'ingénierie inverse de cette capacité. L'un des premiers algorithmes biologiquement inspirés a été formulé par Donald Hebb (1904–1985). Dans son livre révolutionnaire _The Organization of Behavior_ (Hebb, 1949), il a postulé que les neurones apprennent par renforcement positif. Cela est devenu connu sous le nom de règle d'apprentissage de Hebb. Ces idées ont inspiré des travaux ultérieurs, tels que l'algorithme d'apprentissage du perceptron de Rosenblatt, et ont jeté les bases de nombreux algorithmes de descente de gradient stochastique qui sous-tendent l'apprentissage profond aujourd'hui : renforcer le comportement souhaitable et diminuer le comportement indésirable pour obtenir de bons réglages des paramètres dans un réseau de neurones.

L'inspiration biologique est ce qui a donné leur nom aux réseaux de neurones. Depuis plus d'un siècle (remontant aux modèles d'Alexander Bain, 1873, et James Sherrington, 1890), les chercheurs ont essayé d'assembler des circuits computationnels qui ressemblent à des réseaux de neurones en interaction. Au fil du temps, l'interprétation de la biologie est devenue moins littérale, mais le nom est resté. En son cœur se trouvent quelques principes clés que l'on retrouve dans la plupart des réseaux aujourd'hui :

- L'alternance d'unités de traitement linéaires et non linéaires, souvent appelées _couches_.
- L'utilisation de la règle de la chaîne (également connue sous le nom de rétropropagation) pour ajuster les paramètres dans tout le réseau à la fois.

Après des progrès rapides initiaux, la recherche sur les réseaux de neurones a langui d'environ 1995 jusqu'à 2005. Cela était principalement dû à deux raisons. Premièrement, l'entraînement d'un réseau est très coûteux en calcul. Alors que la mémoire vive était abondante à la fin du siècle dernier, la puissance de calcul était rare. Deuxièmement, les jeux de données étaient relativement petits. En fait, le jeu de données Iris de Fisher de 1936 était encore un outil populaire pour tester l'efficacité des algorithmes. Le jeu de données MNIST avec ses 60 000 chiffres manuscrits était considéré comme énorme.

Compte tenu de la rareté des données et du calcul, des outils statistiques puissants tels que les méthodes à noyaux, les arbres de décision et les modèles graphiques se sont avérés empiriquement supérieurs dans de nombreuses applications. De plus, contrairement aux réseaux de neurones, ils ne nécessitaient pas des semaines d'entraînement et fournissaient des résultats prévisibles avec de fortes garanties théoriques.

## 1.5 La route vers l'apprentissage profond

Beaucoup de cela a changé avec la disponibilité de quantités massives de données, grâce au World Wide Web, à l'avènement d'entreprises servant des centaines de millions d'utilisateurs en ligne, à une diffusion de capteurs de haute qualité à faible coût, au stockage de données bon marché (loi de Kryder) et au calcul bon marché (loi de Moore). En particulier, le paysage du calcul en apprentissage profond a été révolutionné par les progrès des GPU qui étaient à l'origine conçus pour les jeux informatiques. Soudain, des algorithmes et des modèles qui semblaient irréalisables sur le plan computationnel étaient à portée de main.

Notez que la mémoire vive n'a pas suivi le rythme de la croissance des données. En même temps, les augmentations de la puissance de calcul ont dépassé la croissance des jeux de données. Cela signifie que les modèles statistiques doivent devenir plus efficaces en mémoire, et ils sont donc libres de passer plus de cycles informatiques à optimiser les paramètres, grâce à l'augmentation du budget de calcul. Par conséquent, le point idéal en apprentissage automatique et en statistiques est passé des modèles linéaires (généralisés) et des méthodes à noyaux aux réseaux de neurones profonds. C'est aussi l'une des raisons pour lesquelles de nombreux piliers de l'apprentissage profond, tels que les perceptrons multicouches (McCulloch et Pitts, 1943), les réseaux de neurones convolutionnels (LeCun et al., 1998), la mémoire à long court terme (LSTM) (Hochreiter et Schmidhuber, 1997) et le Q-Learning (Watkins et Dayan, 1992), ont été essentiellement « redécouverts » au cours de la dernière décennie, après être restés relativement dormants pendant un temps considérable.

Les progrès récents dans les modèles statistiques, les applications et les algorithmes ont parfois été comparés à l'explosion cambrienne : un moment de progrès rapide dans l'évolution des espèces. En effet, l'état de l'art n'est pas simplement une conséquence des ressources disponibles appliquées à des algorithmes vieux de plusieurs décennies. Notez que la liste d'idées ci-dessous effleure à peine la surface de ce qui a aidé les chercheurs à réaliser d'énormes progrès au cours de la dernière décennie.

- De nouvelles méthodes pour le contrôle de la capacité, telles que le _dropout_ (Srivastava et al., 2014), ont aidé à atténuer le surapprentissage. Ici, du bruit est injecté (Bishop, 1995) dans tout le réseau de neurones pendant l'entraînement.
- Les mécanismes d'attention ont résolu un deuxième problème qui tourmentait les statistiques depuis plus d'un siècle : comment augmenter la mémoire et la complexité d'un système sans augmenter le nombre de paramètres apprenables. Les chercheurs ont trouvé une solution élégante en utilisant ce qui ne peut être vu que comme une structure de pointeur apprenable (Bahdanau et al., 2014). Plutôt que d'avoir à se souvenir d'une séquence de texte entière, par exemple pour la traduction automatique dans une représentation de dimension fixe, tout ce qui devait être stocké était un pointeur vers l'état intermédiaire du processus de traduction. Cela a permis une précision considérablement accrue pour les longues séquences, puisque le modèle n'avait plus besoin de se souvenir de la séquence entière avant de commencer la génération d'une nouvelle.
- Construit uniquement sur des mécanismes d'attention, l'architecture Transformer (Vaswani et al., 2017) a démontré un comportement de mise à l'échelle supérieur : elle performe mieux avec une augmentation de la taille du jeu de données, de la taille du modèle et de la quantité de calcul d'entraînement (Kaplan et al., 2020). Cette architecture a démontré un succès convaincant dans un large éventail de domaines, tels que le traitement du langage naturel (Brown et al., 2020, Devlin et al., 2018), la vision par ordinateur (Dosovitskiy et al., 2021, Liu et al., 2021), la reconnaissance vocale (Gulati et al., 2020), l'apprentissage par renforcement (Chen et al., 2021) et les réseaux de neurones graphiques (Dwivedi et Bresson, 2020). Par exemple, un seul Transformer pré-entraîné sur des modalités aussi diverses que le texte, les images, les couples articulaires et les pressions de boutons peut jouer à Atari, légender des images, discuter et contrôler un robot (Reed et al., 2022).
- Modélisant les probabilités de séquences de texte, les modèles de langage peuvent prédire le texte étant donné d'autres textes. La mise à l'échelle des données, du modèle et du calcul a débloqué un nombre croissant de capacités des modèles de langage pour effectuer des tâches souhaitées via la génération de texte de type humain basée sur le texte d'entrée. Par exemple, en alignant les modèles de langage avec l'intention humaine (Ouyang et al., 2022), ChatGPT d'OpenAI permet aux utilisateurs d'interagir avec lui de manière conversationnelle pour résoudre des problèmes, tels que le débogage de code et l'écriture créative.
- Les conceptions multi-étapes, par exemple via les réseaux de mémoire (Sukhbaatar et al., 2015) et le programmeur-interpréteur neuronal (Reed et De Freitas, 2015) ont permis aux modélisateurs statistiques de décrire des approches itératives du raisonnement. Ces outils permettent à un état interne du réseau de neurones profond d'être modifié à plusieurs reprises, effectuant ainsi des étapes ultérieures dans une chaîne de raisonnement, tout comme un processeur peut modifier la mémoire pour un calcul.
- Un développement clé dans la modélisation générative profonde a été l'invention des réseaux antagonistes génératifs (GANs) (Goodfellow et al., 2014). Traditionnellement, les méthodes statistiques pour l'estimation de densité et les modèles génératifs se concentraient sur la recherche de distributions de probabilité appropriées et d'algorithmes (souvent approximatifs) pour l'échantillonnage à partir de celles-ci. En conséquence, ces algorithmes étaient largement limités par le manque de flexibilité inhérent aux modèles statistiques. L'innovation cruciale dans les réseaux antagonistes génératifs a été de remplacer l'échantillonneur par un algorithme arbitraire avec des paramètres différentiables. Ceux-ci sont ensuite ajustés de telle manière que le discriminateur (effectivement un test à deux échantillons) ne peut pas distinguer les fausses données des vraies. Grâce à la capacité d'utiliser des algorithmes arbitraires pour générer des données, l'estimation de densité a été ouverte à une grande variété de techniques. Des exemples de zèbres galopants (Zhu et al., 2017) et de faux visages de célébrités (Karras et al., 2017) témoignent chacun de ces progrès. Même les gribouilleurs amateurs peuvent produire des images photoréalistes simplement basées sur des croquis décrivant la disposition d'une scène (Park et al., 2019).
- De plus, alors que le processus de diffusion ajoute progressivement du bruit aléatoire aux échantillons de données, les modèles de diffusion (Ho et al., 2020, Sohl-Dickstein et al., 2015) apprennent le processus de débruitage pour construire progressivement des échantillons de données à partir de bruit aléatoire, inversant le processus de diffusion. Ils ont commencé à remplacer les réseaux antagonistes génératifs dans les modèles génératifs profonds plus récents, tels que dans DALL-E 2 (Ramesh et al., 2022) et Imagen (Saharia et al., 2022) pour l'art créatif et la génération d'images basés sur des descriptions textuelles.
- Dans de nombreux cas, un seul GPU est insuffisant pour traiter les grandes quantités de données disponibles pour l'entraînement. Au cours de la dernière décennie, la capacité à construire des algorithmes d'entraînement parallèles et distribués s'est considérablement améliorée. L'un des défis clés dans la conception d'algorithmes évolutifs est que le cheval de bataille de l'optimisation de l'apprentissage profond, la descente de gradient stochastique, repose sur des mini-lots de données relativement petits à traiter. En même temps, les petits lots limitent l'efficacité des GPU. Par conséquent, l'entraînement sur 1 024 GPU avec une taille de mini-lot de, disons, 32 images par lot équivaut à un mini-lot global d'environ 32 000 images. Les travaux, d'abord par Li (2017) et ensuite par You et al. (2017) et Jia et al. (2018) ont poussé la taille jusqu'à 64 000 observations, réduisant le temps d'entraînement pour le modèle ResNet-50 sur le jeu de données ImageNet à moins de 7 minutes. En comparaison, les temps d'entraînement étaient initialement de l'ordre de plusieurs jours.
- La capacité de paralléliser le calcul a également contribué aux progrès de l'apprentissage par renforcement. Cela a conduit à des progrès significatifs dans l'atteinte par les ordinateurs de performances surhumaines sur des tâches comme le Go, les jeux Atari, Starcraft, et dans les simulations physiques (par exemple, en utilisant MuJoCo) où des simulateurs d'environnement sont disponibles. Voir, par exemple, Silver et al. (2016) pour une description de telles réalisations dans AlphaGo. En résumé, l'apprentissage par renforcement fonctionne mieux si de nombreux tuples (état, action, récompense) sont disponibles. La simulation fournit une telle voie.
- Les frameworks d'apprentissage profond ont joué un rôle crucial dans la diffusion des idées. La première génération de frameworks open source pour la modélisation de réseaux de neurones comprenait Caffe, Torch et Theano. De nombreux articles fondateurs ont été écrits à l'aide de ces outils. Ceux-ci ont maintenant été remplacés par TensorFlow (souvent utilisé via son API de haut niveau Keras), CNTK, Caffe 2 et Apache MXNet. La troisième génération de frameworks consiste en des outils dits impératifs pour l'apprentissage profond, une tendance qui a sans doute été déclenchée par Chainer, qui utilisait une syntaxe similaire à Python NumPy pour décrire les modèles. Cette idée a été adoptée par PyTorch, l'API Gluon de MXNet, et JAX.

La division du travail entre les chercheurs en systèmes construisant de meilleurs outils et les modélisateurs statistiques construisant de meilleurs réseaux de neurones a grandement simplifié les choses. Par exemple, l'entraînement d'un modèle de régression logistique linéaire était autrefois un devoir non trivial, digne d'être donné aux nouveaux doctorants en apprentissage automatique à l'Université Carnegie Mellon en 2014. À l'heure actuelle, cette tâche peut être accomplie avec moins de 10 lignes de code, la mettant fermement à la portée de tout programmeur.

## 1.6 Histoires de réussite

L'intelligence artificielle a une longue histoire de production de résultats qui seraient difficiles à accomplir autrement. Par exemple, des systèmes de tri du courrier utilisant la reconnaissance optique de caractères sont déployés depuis les années 1990. C'est, après tout, la source du célèbre jeu de données MNIST de chiffres manuscrits. Il en va de même pour la lecture des chèques pour les dépôts bancaires et la notation de la solvabilité des demandeurs. Les transactions financières sont vérifiées automatiquement pour détecter la fraude. Cela forme l'épine dorsale de nombreux systèmes de paiement de commerce électronique, tels que PayPal, Stripe, AliPay, WeChat, Apple, Visa et MasterCard. Les programmes informatiques pour les échecs sont compétitifs depuis des décennies. L'apprentissage automatique alimente la recherche, la recommandation, la personnalisation et le classement sur Internet. En d'autres termes, l'apprentissage automatique est omniprésent, bien que souvent caché de la vue.

Ce n'est que récemment que l'IA a été sous les projecteurs, principalement en raison de solutions à des problèmes qui étaient considérés comme insolubles auparavant et qui sont directement liés aux consommateurs. Beaucoup de ces avancées sont attribuées à l'apprentissage profond.

- Les assistants intelligents, tels que Siri d'Apple, Alexa d'Amazon et l'assistant de Google, sont capables de répondre aux demandes orales avec un degré de précision raisonnable. Cela inclut des tâches serviles, comme allumer des interrupteurs, et des tâches plus complexes, telles que l'organisation de rendez-vous chez le coiffeur et l'offre d'un dialogue d'assistance téléphonique. C'est probablement le signe le plus visible que l'IA affecte nos vies.
- Un ingrédient clé des assistants numériques est leur capacité à reconnaître la parole avec précision. La précision de tels systèmes a progressivement augmenté au point d'atteindre la parité avec les humains pour certaines applications (Xiong et al., 2018).
- La reconnaissance d'objets a également parcouru un long chemin. Identifier l'objet dans une image était une tâche assez difficile en 2010. Sur le benchmark ImageNet, des chercheurs de NEC Labs et de l'Université de l'Illinois à Urbana-Champaign ont atteint un taux d'erreur top-5 de 28 % (Lin et al., 2010). En 2017, ce taux d'erreur a été réduit à 2,25 % (Hu et al., 2018). De même, des résultats étonnants ont été obtenus pour l'identification du chant des oiseaux et pour le diagnostic du cancer de la peau.
- Les prouesses dans les jeux servaient de bâton de mesure pour la capacité humaine. Partant de TD-Gammon, un programme pour jouer au backgammon utilisant l'apprentissage par renforcement par différence temporelle, les progrès algorithmiques et computationnels ont conduit à des algorithmes pour un large éventail d'applications. Par rapport au backgammon, les échecs ont un espace d'états et un ensemble d'actions beaucoup plus complexes. DeepBlue a battu Garry Kasparov en utilisant un parallélisme massif, du matériel spécialisé et une recherche efficace à travers l'arbre de jeu (Campbell et al., 2002). Le Go est encore plus difficile, en raison de son énorme espace d'états. AlphaGo a atteint la parité humaine en 2015, en utilisant l'apprentissage profond combiné à l'échantillonnage d'arbre Monte Carlo (Silver et al., 2016). Le défi au Poker était que l'espace d'états est grand et seulement partiellement observé (nous ne connaissons pas les cartes des adversaires). Libratus a dépassé la performance humaine au Poker en utilisant des stratégies structurées efficacement (Brown et Sandholm, 2017).
- Une autre indication des progrès de l'IA est l'avènement des véhicules autonomes. Bien que l'autonomie complète ne soit pas encore à portée de main, d'excellents progrès ont été réalisés dans cette direction, avec des entreprises telles que Tesla, NVIDIA et Waymo expédiant des produits qui permettent une autonomie partielle. Ce qui rend l'autonomie complète si difficile, c'est qu'une conduite correcte nécessite la capacité de percevoir, de raisonner et d'incorporer des règles dans un système. À l'heure actuelle, l'apprentissage profond est utilisé principalement dans l'aspect visuel de ces problèmes. Le reste est fortement réglé par des ingénieurs.

## 1.7 L'essence de l'apprentissage profond

Jusqu'à présent, nous avons parlé en termes généraux de l'apprentissage automatique. L'apprentissage profond est le sous-ensemble de l'apprentissage automatique concerné par les modèles basés sur des réseaux de neurones à plusieurs couches. Il est _profond_ précisément dans le sens où ses modèles apprennent de nombreuses _couches_ de transformations. Bien que cela puisse sembler étroit, l'apprentissage profond a donné naissance à une gamme vertigineuse de modèles, de techniques, de formulations de problèmes et d'applications. De nombreuses intuitions ont été développées pour expliquer les avantages de la profondeur. On peut dire que tout l'apprentissage automatique comporte de nombreuses couches de calcul, la première consistant en des étapes de traitement des caractéristiques. Ce qui différencie l'apprentissage profond, c'est que les opérations apprises à chacune des nombreuses couches de représentations sont apprises conjointement à partir des données.

Les problèmes dont nous avons discuté jusqu'à présent, tels que l'apprentissage à partir du signal audio brut, des valeurs brutes de pixels des images, ou le mappage entre des phrases de longueurs arbitraires et leurs homologues dans des langues étrangères, sont ceux où l'apprentissage profond excelle et où les méthodes traditionnelles échouent. Il s'avère que ces modèles multicouches sont capables de traiter des données perceptuelles de bas niveau d'une manière que les outils précédents ne pouvaient pas. Probablement la caractéristique commune la plus significative des méthodes d'apprentissage profond est l'_entraînement de bout en bout_. C'est-à-dire que plutôt que d'assembler un système basé sur des composants qui sont réglés individuellement, on construit le système et on règle ensuite leurs performances conjointement. Par exemple, en vision par ordinateur, les scientifiques avaient l'habitude de séparer le processus d'ingénierie des caractéristiques du processus de construction de modèles d'apprentissage automatique. Le détecteur de contours de Canny (Canny, 1987) et l'extracteur de caractéristiques SIFT de Lowe (Lowe, 2004) ont régné en maîtres pendant plus d'une décennie comme algorithmes pour mapper des images en vecteurs de caractéristiques. Autrefois, la partie cruciale de l'application de l'apprentissage automatique à ces problèmes consistait à trouver des moyens conçus manuellement de transformer les données en une forme adaptée aux modèles peu profonds. Malheureusement, il y a une limite à ce que les humains peuvent accomplir par ingéniosité par rapport à une évaluation cohérente sur des millions de choix effectuée automatiquement par un algorithme. Lorsque l'apprentissage profond a pris le dessus, ces extracteurs de caractéristiques ont été remplacés par des filtres réglés automatiquement qui ont donné une précision supérieure.

Ainsi, un avantage clé de l'apprentissage profond est qu'il remplace non seulement les modèles peu profonds à la fin des pipelines d'apprentissage traditionnels, mais aussi le processus laborieux d'ingénierie des caractéristiques. De plus, en remplaçant une grande partie du prétraitement spécifique au domaine, l'apprentissage profond a éliminé de nombreuses frontières qui séparaient auparavant la vision par ordinateur, la reconnaissance vocale, le traitement du langage naturel, l'informatique médicale et d'autres domaines d'application, offrant ainsi un ensemble d'outils unifié pour aborder divers problèmes.

Au-delà de l'entraînement de bout en bout, nous vivons une transition des descriptions statistiques paramétriques vers des modèles entièrement non paramétriques. Lorsque les données sont rares, il faut s'appuyer sur des hypothèses simplificatrices sur la réalité afin d'obtenir des modèles utiles. Lorsque les données sont abondantes, celles-ci peuvent être remplacées par des modèles non paramétriques qui s'adaptent mieux aux données. Dans une certaine mesure, cela reflète les progrès que la physique a connus au milieu du siècle précédent avec la disponibilité des ordinateurs. Plutôt que de résoudre à la main des approximations paramétriques du comportement des électrons, on peut maintenant recourir à des simulations numériques des équations aux dérivées partielles associées. Cela a conduit à des modèles beaucoup plus précis, bien que souvent au détriment de l'interprétation.

Une autre différence par rapport aux travaux précédents est l'acceptation de solutions sous-optimales, traitant de problèmes d'optimisation non linéaires non convexes, et la volonté d'essayer des choses avant de les prouver. Ce nouvel empirisme dans le traitement des problèmes statistiques, combiné à un afflux rapide de talents, a conduit à des progrès rapides dans le développement d'algorithmes pratiques, bien que dans de nombreux cas au détriment de la modification et de la réinvention d'outils qui existaient depuis des décennies.

En fin de compte, la communauté de l'apprentissage profond est fière de partager des outils au-delà des frontières académiques et corporatives, publiant de nombreuses excellentes bibliothèques, modèles statistiques et réseaux entraînés en open source. C'est dans cet esprit que les notebooks formant ce livre sont librement disponibles pour la distribution et l'utilisation. Nous avons travaillé dur pour abaisser les barrières d'accès pour quiconque souhaite apprendre l'apprentissage profond et nous espérons que nos lecteurs en bénéficieront.

## 1.8 Résumé

L'apprentissage automatique étudie comment les systèmes informatiques peuvent tirer parti de l'expérience (souvent des données) pour améliorer les performances à des tâches spécifiques. Il combine des idées issues des statistiques, de l'exploration de données et de l'optimisation. Souvent, il est utilisé comme moyen de mettre en œuvre des solutions d'IA. En tant que classe d'apprentissage automatique, l'apprentissage représentationnel se concentre sur la manière de trouver automatiquement la façon appropriée de représenter les données. Considéré comme un apprentissage de représentation à plusieurs niveaux par l'apprentissage de nombreuses couches de transformations, l'apprentissage profond remplace non seulement les modèles peu profonds à la fin des pipelines d'apprentissage automatique traditionnels, mais aussi le processus laborieux d'ingénierie des caractéristiques. Une grande partie des progrès récents en apprentissage profond a été déclenchée par une abondance de données provenant de capteurs bon marché et d'applications à l'échelle d'Internet, et par des progrès significatifs en calcul, principalement grâce aux GPU. De plus, la disponibilité de frameworks d'apprentissage profond efficaces a rendu la conception et la mise en œuvre de l'optimisation de systèmes entiers considérablement plus faciles, et c'est un composant clé pour obtenir de hautes performances.

## 1.9 Exercices

1. Quelles parties du code que vous écrivez actuellement pourraient être « apprises », c'est-à-dire améliorées par l'apprentissage et la détermination automatique des choix de conception qui sont faits dans votre code ? Votre code inclut-il des choix de conception heuristiques ? De quelles données pourriez-vous avoir besoin pour apprendre le comportement souhaité ?
2. Quels problèmes rencontrez-vous qui ont de nombreux exemples pour leur solution, mais aucun moyen spécifique de les automatiser ? Ceux-ci peuvent être des candidats de choix pour l'utilisation de l'apprentissage profond.
3. Décrivez les relations entre les algorithmes, les données et le calcul. Comment les caractéristiques des données et les ressources de calcul actuellement disponibles influencent-elles la pertinence de divers algorithmes ?
4. Nommez quelques contextes où l'entraînement de bout en bout n'est pas actuellement l'approche par défaut mais où il pourrait être utile.
